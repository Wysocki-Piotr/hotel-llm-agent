llm:
  provider: "ollama"
  model_name: "llama3.1"
  temperature: 0.3
  base_url: "http://localhost:11434" # Dla providera 'openai_compatible' (VM) lub Ollama
  num_gpu: 99, #wzięte z nikąd ale działa, coś koło 40 wystarczy
  num_ctx: 4096 #2048 bylo za małe, 8192 wolne

database:
  url: "sqlite:///data/hotels.db"

logging:
  level: "INFO"
  file: "logs/agent.log"