llm:
  llm:
  provider: "groq"
  model_name: "llama-3.3-70b-versatile"  # To jest ten potężny, szybki model
  temperature: 0.1
  base_url: "http://localhost:11434" # Dla providera 'openai_compatible' (VM) lub Ollama
  num_gpu: 99

database:
  url: "sqlite:///data/hotels.db"

logging:
  level: "INFO"
  file: "logs/agent.log"